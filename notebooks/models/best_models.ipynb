{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339d7bee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Diplomatura en Ciencias de Datos, Aprendizaje Automático y sus Aplicaciones\n",
    "\n",
    "Autores: Matías Oria, Antonela Sambuceti, Pamela Pairo, Benjamín Ocampo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed3248",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Mejores 3 Modelos de la Competencia\n",
    "Se trabajó sobre los siguientes modelos:\n",
    "\n",
    "- `xgb`: XGBoost.\n",
    "- `lgr`: Regresión logistica.\n",
    "- `svm`: Support Vector Machines.\n",
    "- `rndforest`: Random Forest.\n",
    "- `dtree`: Decision Tree.\n",
    "- `sgd`: SGD Classifier para regresión logistica.\n",
    "- `nnet`: Neural Networks (Sklearn).\n",
    "- `nnet_keras`: Neural Networks (Keras).\n",
    "\n",
    "De los cuales los `xgb`, `svm`, y `rndforest` lograron los mejores resultados\n",
    "para el conjunto de *test* público en la competencia de Kaggle.\n",
    "\n",
    "La implementación y busqueda de hiperparametros del resto de modelos puede\n",
    "encontrarse en una notebook dedicada para cada uno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b05a8f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Pipeline\n",
    "El pipeline planteado durante el proceso de aprendizaje consiste de 3 capas.\n",
    "\n",
    "- Discretización para las variables continuas `Àge`, y `ÀnnualIncome`.\n",
    "- Códificación one-hot para el resto de variables categoricas.\n",
    "- Modelo.\n",
    "\n",
    "Donde las primeras dos son de preprocesamiento, mientras que la tercera es el\n",
    "predictor utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (make_pipeline, save_predictions, X_train_total,\n",
    "                   y_train_total, X_train, X_val, y_train, y_val)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a8368",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1º XGBoost\n",
    "Para este modelo se optó por realizar una busqueda de hiperparámetros\n",
    "utilizando todas las filas y columnas del conjunto de entrenamiento por medio\n",
    "de una Busqueda Bayesiana (`Bayesian Search`) donde se especifica un rango o\n",
    "espacio sobre los parametros que se desean encontrar. Este espacio se recorre\n",
    "de manera apropiada recordando iteraciones pasadas de busqueda hasta encontrar\n",
    "el modelo óptimo. Algo a recalcar es que no solamente se ajustó la signature\n",
    "del clasificador, si no que también se intentó encontrar distintas\n",
    "discretizaciones posibles durante el preprocesamiento. \n",
    "\n",
    "Resultado en Kaggle f1-score: 0.80898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c39296",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgboost = make_pipeline(XGBClassifier())\n",
    "params_bayes = {\n",
    "    \"preprocessor__discretizer__n_bins\": Integer(2, 25),\n",
    "    \"model__objective\": Categorical([\"binary:logistic\"]),\n",
    "    \"model__n_estimators\": Integer(10, 500),\n",
    "    \"model__gamma\": Real(1e-6, 1),\n",
    "    \"model__max_depth\": Integer(4, 20),\n",
    "    \"model__learning_rate\": Real(1e-4, 1),\n",
    "    \"model__alpha\": Real(1e-4, 1),\n",
    "    \"model__booster\": Categorical([\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "    \"model__colsample_bytree\": Real(.5, 1),\n",
    "    \"model__subsample\": Real(.6, 1),\n",
    "    \"model__eval_metric\": Categorical([\"logloss\"]),\n",
    "    \"model__use_label_encoder\": Categorical([False]),\n",
    "}\n",
    "opt = BayesSearchCV(xgboost, params_bayes, scoring=\"f1\")\n",
    "opt.fit(X_train_total, y_train_total)\n",
    "save_predictions(opt.best_estimator_, \"xgb.csv\")\n",
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41abcf5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 2º SVM\n",
    "El segundo modelo consta de una busqueda de hiperparametros por medio de una\n",
    "Grid Search recorriendo sobre 4 kernels posibles, `linear`, `sigmoid`, `poly`,\n",
    "`rbf`. A diferencia de `xgb`, en este caso se optó por dividir los datos en\n",
    "train y validación para obtener métricas del modelo sobre este último conjunto.\n",
    "Una vez realizada la busqueda de hiperparametros sobre el conjunto de datos de\n",
    "entrenamiento, se utilizan los mejores hiperparametros para realizar un último\n",
    "ajuste con `train` + `validation`.\n",
    "\n",
    "Resultado en Kaggle f1-score: 0.80459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ed5f1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = make_pipeline(SVC())\n",
    "\n",
    "params = {\n",
    "    \"model__kernel\": [\"linear\", \"sigmoid\", \"poly\", \"rbf\"],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"],\n",
    "    \"model__degree\": [2, 3, 4],\n",
    "    \"model__coef0\": [.001, .01, 0, 1],\n",
    "    \"model__tol\": [1e-2, 1e-3, 1e-4],\n",
    "    \"model__C\": [1, 0.1, 0.01, 0.001, 0.0001, 10],\n",
    "    \"model__class_weight\": [\"balanced\"]\n",
    "}\n",
    "clf = GridSearchCV(svm, param_grid=params, scoring=\"f1\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_val_pred = clf.best_estimator_.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11ec4d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Refit with validation data\n",
    "best_model = make_pipeline(\n",
    "    SVC(\n",
    "        **{\n",
    "            key.removeprefix(\"model__\"): value\n",
    "            for key, value in clf.best_params_.items()\n",
    "        }))\n",
    "best_model.fit(X_train_total, y_train_total)\n",
    "save_predictions(best_model, \"svm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f3149",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 3º Random Forest\n",
    "En este último caso, de manera similar a `xgb`, se optó por utilizar Busqueda\n",
    "Bayesiana sin dividir los datos de entrenamiento.\n",
    "\n",
    "Resultado en Kaggle f1-score: 0.79120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe = make_pipeline(RandomForestClassifier())\n",
    "params_bayes = {\n",
    "    \"preprocessor__discretizer__n_bins\": Integer(2, 25),\n",
    "    \"model__max_features\": Categorical([\"auto\", \"sqrt\", \"log2\"]),\n",
    "    \"model__n_estimators\": Integer(10, 500),\n",
    "    \"model__max_depth\": Integer(1, 100),\n",
    "    \"model__min_samples_split\": Integer(2, 100),\n",
    "    \"model__min_samples_leaf\": Integer(1, 100),\n",
    "    \"model__bootstrap\": Categorical([True, False])\n",
    "}\n",
    "opt = BayesSearchCV(pipe, params_bayes, scoring=\"f1\")\n",
    "opt.fit(X_train_total, y_train_total)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
